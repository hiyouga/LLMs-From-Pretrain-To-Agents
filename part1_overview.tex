% Part 1: Technical Landscape
\section{Technical Landscape}

\begin{frame}
\frametitle{Transformer Architecture}
\begin{itemize}
    \item \textbf{"Attention is All You Need"} (Vaswani et al., 2017)
    \item Self-attention mechanism: captures long-range dependencies
    \item Key components:
    \begin{itemize}
        \item Multi-head attention
        \item Positional encoding
        \item Feed-forward networks
        \item Layer normalization
    \end{itemize}
    \item Advantages:
    \begin{itemize}
        \item Parallel processing (vs. sequential RNNs)
        \item Flexible context modeling
        \item Scalable architecture
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Probabilistic Modeling}
\begin{itemize}
    \item Language modeling as probability distribution: $P(w_1, w_2, ..., w_n)$
    \item Autoregressive formulation: $P(w_t | w_1, ..., w_{t-1})$
    \item Training objectives:
    \begin{itemize}
        \item Autoregressive language modeling
        \item Masked language modeling
        \item Diffusion models
    \end{itemize}
    \item Maximum likelihood estimation via cross-entropy loss
    \item Foundation for generation and understanding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Scaling Laws}
\begin{itemize}
    \item \textbf{Key insight}: Model performance scales predictably with:
    \begin{itemize}
        \item Model size (parameters)
        \item Dataset size (tokens)
        \item Compute budget (FLOPs)
    \end{itemize}
    \item Power law relationships (Kaplan et al., 2020)
    \item Chinchilla scaling laws (Hoffmann et al., 2022):
    \begin{itemize}
        \item Optimal ratio of model size to training data
        \item Compute-optimal training
    \end{itemize}
    \item Implications: Bigger models, more data = better performance
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Emergent Abilities}
\begin{itemize}
    \item Capabilities that appear at scale, absent in smaller models
    \item Examples:
    \begin{itemize}
        \item Few-shot learning (GPT-3)
        \item Chain-of-thought reasoning
        \item Instruction following
        \item Multi-step problem solving
        \item Arithmetic and symbolic reasoning
    \end{itemize}
    \item Not explicitly trained, emerge from scale
    \item Phase transitions in capability vs. scale
    \item Critical for advanced applications
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LLM and Agent Development Roadmap}
\begin{center}
\begin{tikzpicture}[scale=1.0, every node/.style={scale=0.9}]
    % Axes
    \draw[->, thick] (0,0) -- (8,0) node[right] {Model Capability};
    \draw[->, thick] (0,0) -- (0,6) node[above] {Agent Environment};
    
    % Origin point
    \fill (0,0) circle (3pt) node[below left] {Origin};
    
    % Model development path (horizontal axis) - Long Reasoning
    \draw[->, very thick, blue] (0,0) -- (7,0);
    \node[blue, below] at (3.5,-0.3) {Long Reasoning};
    
    % Agent development path (vertical axis) - Comprehensive Environment
    \draw[->, very thick, green!60!black] (0,0) -- (0,5);
    \node[green!60!black, rotate=90, above] at (-0.4,2.5) {Comprehensive Environment};
\end{tikzpicture}
\end{center}
\end{frame}
