% Part 1: Evolutionary Pathways of Large Language Models
\section{Evolutionary Pathways of Large Language Models}

\begin{frame}
\frametitle{Transformer Architecture}
\begin{itemize}
    \item \textbf{"Attention is All You Need"} (Vaswani et al., 2017)
    \item Self-attention mechanism: captures long-range dependencies
    \item Key components:
    \begin{itemize}
        \item Multi-head attention
        \item Positional encoding
        \item Feed-forward networks
        \item Layer normalization
    \end{itemize}
    \item Technical evolution of attention mechanisms:
    \begin{itemize}
        \item From dense to sparse attention: MQA, GQA, MLA, DSA
        \item From dense to sparse architectures: MoE (Mixture-of-Experts)
        \item Towards fine-grained MoE specialization
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Probabilistic Modeling}
\begin{itemize}
    \item Word-level probability prediction: $P(w_1, w_2, ..., w_n)$
    \item Current training paradigms:
    \begin{itemize}
        \item Autoregressive language modeling
        \item Diffusion models
    \end{itemize}
    \item Training methods:
    \begin{itemize}
        \item Pre-training: Learning from large-scale data
        \item Supervised Fine-Tuning (SFT): Task-specific adaptation
        \item Direct Preference Optimization (DPO)
        \item Proximal Policy Optimization (PPO)
        \item Group Relative Policy Optimization (GRPO)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Scaling Laws}
\begin{columns}
\column{0.5\textwidth}
\begin{itemize}
    \item Power law relationship:
    \[
    L(N, D, C) \approx A N^{-\alpha} + B D^{-\beta} + E
    \]
    \item \textbf{Key variables}:
    \begin{itemize}
        \item $N$: Model size (parameters)
        \item $D$: Dataset size (tokens)
        \item $C$: Compute budget (FLOPs)
    \end{itemize}
    \item Optimal compute allocation strategies
\end{itemize}
\column{0.5\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{images/scaling_law.pdf}
\end{center}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Emergent Abilities}
\begin{itemize}
    \item Capabilities that appear at scale, absent in smaller models
    \item Key emergent behaviors:
    \begin{itemize}
        \item Few-shot learning
        \item Zero-shot learning
        \item Chain-of-thought reasoning
        \item ReAct loop (reasoning and acting)
        \item Interleaved thinking
    \end{itemize}
    \item Not explicitly trained, emerge from scale
    \item Phase transitions in capability vs. scale
    \item Critical for advanced applications
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{LLM and Agent Development Roadmap}
\begin{center}
\begin{tikzpicture}[scale=0.7, every node/.style={scale=0.7}]
    % Define the size of the square
    \def\maxsize{6}
    
    % Fill upper-left triangle (Comprehensive Environment)
    \fill[green!20] (0,0) -- (0,\maxsize) -- (\maxsize,\maxsize) -- cycle;
    
    % Fill lower-right triangle (Long Reasoning)
    \fill[blue!20] (0,0) -- (\maxsize,0) -- (\maxsize,\maxsize) -- cycle;
    
    % Draw the diagonal line y=x
    \draw[thick, dashed] (0,0) -- (\maxsize,\maxsize);
    
    % Axes
    \draw[->, thick] (0,0) -- (\maxsize+1,0) node[right] {Model Capability};
    \draw[->, thick] (0,0) -- (0,\maxsize+1) node[above] {Agent Environment};
    
    % Origin point (without text label)
    \fill (0,0) circle (3pt);
    
    % Labels for regions
    \node[green!60!black, align=center] at (1.5,4.5) {Comprehensive\\Environment};
    \node[blue!60!black, align=center] at (4.5,1.5) {Long\\Reasoning};
\end{tikzpicture}
\end{center}
\end{frame}
