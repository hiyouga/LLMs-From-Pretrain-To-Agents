% Part 1: Evolutionary Pathways of Large Language Models
\section{Evolutionary Pathways of Large Language Models}

\begin{frame}
\frametitle{Transformer Architecture}
\begin{itemize}
    \item \textbf{``Attention is All You Need''} (Vaswani et al., 2017)
    \item Self-attention mechanism: captures long-range dependencies
    \item Key components:
    \begin{itemize}
        \item Multi-head attention
        \item Positional encoding
        \item Feed-forward networks
        \item Layer normalization
    \end{itemize}
    \item Technical evolution of transformer architectures:
    \begin{itemize}
        \item From dense to sparse attention: MHA $\rightarrow$ GQA $\rightarrow$ MLA $\rightarrow$ DSA
        \item From dense to sparse MLP: Dense FFN $\rightarrow$ MoE $\rightarrow$ fine-grained MoE
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Probabilistic Modeling}
\begin{itemize}
    \item Word-level probability prediction: $P(w_1, w_2, ..., w_n)$
    \item Current training paradigms:
    \begin{itemize}
        \item Autoregressive language modeling
        \item Diffusion models
    \end{itemize}
    \item Training methods: Pre-training, SFT, PPO, DPO, GRPO
    \item Trends:
    \begin{itemize}
        \item From expert knowledge to autonomous exploration: RLHF $\rightarrow$ RLVR
        \item From off-policy to on-policy learning: DPO $\rightarrow$ GRPO
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Scaling Laws for LLMs}
\begin{columns}
\column{0.4\textwidth}
\begin{itemize}
    \item Power law relationship:
    \begin{equation*}
    L(N, D, C) \approx A N^{-\alpha} + B D^{-\beta} + E
    \end{equation*}
    \item {\small $N$: Model size (parameters)}
    \item {\small $D$: Dataset size (tokens)}
    \item {\small $C$: Compute budget (FLOPs)}
\end{itemize}
\column{0.6\textwidth}
\begin{center}
\includegraphics[width=\textwidth]{images/scaling_law.pdf}
\end{center}
\end{columns}
\vspace{0.5em}
\begin{center}
\begin{tabular}{lcccc}
\hline
\textbf{Model} & Qwen1 & Qwen2 & Qwen2.5 & Qwen3 \\ \hline
\textbf{Dataset Size}   & 3T    & 7T    & 18T     & 36T   \\ \hline
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Emergent Abilities of LLMs}
\begin{itemize}
    \item Capabilities that appear at scale, absent in smaller models
    \item Key emergent behaviors:
    \begin{itemize}
        \item Few-shot learning (Brown et al., 2020)
        \item Zero-shot learning (Kojima et al., 2022)
        \item Chain-of-thought reasoning (Wei et al., 2022)
        \item Reasoning and acting (Yao et al., 2022)
        \item Interleaved thinking (Unknown, 2025)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Evolution and Future Directions}
\begin{center}
\begin{tikzpicture}[scale=0.7, every node/.style={scale=0.7}]
    % Define the size of the square
    \def\maxsize{6}
    
    % Origin point
    \fill (0,0) circle (3pt) node[below left] {(0,0)};
    
    % Draw orthogonal arrows from (0,0) to (0,1) and (1,0)
    \draw[-{Stealth[length=3mm]}, thick, blue!60] (0,0) -- (0,\maxsize) node[left, midway] {Agent Environment};
    \draw[-{Stealth[length=3mm]}, thick, green!60] (0,0) -- (\maxsize,0) node[below, midway] {Model Capability};
    
    % Draw arrows from (0,1) and (1,0) converging to (1,1)
    \draw[-{Stealth[length=3mm]}, thick, blue!60] (0,\maxsize) -- (\maxsize,\maxsize);
    \draw[-{Stealth[length=3mm]}, thick, green!60] (\maxsize,0) -- (\maxsize,\maxsize);
    
    % Endpoint at (1,1)
    \fill (\maxsize,\maxsize) circle (3pt) node[above right] {(1,1)};
    
    % Mark intermediate points
    \fill (0,\maxsize) circle (2pt) node[above left] {(0,1)};
    \fill (\maxsize,0) circle (2pt) node[below right] {(1,0)};
    
\end{tikzpicture}

\vspace{1em}
\textit{The general trend of the world: what has been divided for a long time will surely unite.}

\textit{天下大势，分久必合。}
\end{center}
\end{frame}

