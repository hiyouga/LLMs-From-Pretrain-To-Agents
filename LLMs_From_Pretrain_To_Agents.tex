\documentclass{beamer}
\usetheme{Luebeck}

\title{LLMs: From Pretrain To Agents}
\author{Yaowei Zheng}
\date{January 26, 2026}

\begin{document}

% Title page
\frame{\titlepage}

% Table of Contents
\begin{frame}
\frametitle{Table of Contents}
\tableofcontents
\end{frame}

% Section 1: Introduction
\section{Introduction to Large Language Models}
\begin{frame}
\frametitle{What are Large Language Models?}
\begin{itemize}
    \item Large Language Models (LLMs) are deep learning models trained on massive text corpora
    \item Capable of understanding and generating human-like text
    \item Built on transformer architecture
    \item Key characteristics:
    \begin{itemize}
        \item Billions of parameters
        \item Pre-trained on diverse datasets
        \item Transfer learning capabilities
    \end{itemize}
\end{itemize}
\end{frame}

% Section 2: Early Days
\section{Early Days: Foundation Models}
\begin{frame}
\frametitle{The Beginning: Word Embeddings}
\begin{itemize}
    \item \textbf{Word2Vec (2013)}: Introduced efficient word embeddings
    \item \textbf{GloVe (2014)}: Global vectors for word representation
    \item \textbf{Key Innovation}: Capturing semantic relationships in vector space
    \item Limitation: Context-independent representations
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The Transformer Revolution (2017)}
\begin{itemize}
    \item \textbf{"Attention is All You Need"} - Vaswani et al.
    \item Self-attention mechanism
    \item Parallel processing capabilities
    \item Foundation for modern LLMs
    \item Key components:
    \begin{itemize}
        \item Multi-head attention
        \item Positional encoding
        \item Feed-forward networks
    \end{itemize}
\end{itemize}
\end{frame}

% Section 3: Pre-training Era
\section{The Pre-training Era}
\begin{frame}
\frametitle{BERT: Bidirectional Representations (2018)}
\begin{itemize}
    \item \textbf{Bidirectional Encoder Representations from Transformers}
    \item Pre-training objectives:
    \begin{itemize}
        \item Masked Language Modeling (MLM)
        \item Next Sentence Prediction (NSP)
    \end{itemize}
    \item Revolutionary for NLU tasks
    \item Encoder-only architecture
    \item Fine-tuning for downstream tasks
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{GPT Series: Autoregressive Models}
\begin{itemize}
    \item \textbf{GPT-1 (2018)}: Generative Pre-Training
    \item \textbf{GPT-2 (2019)}: Scaled to 1.5B parameters
    \item \textbf{GPT-3 (2020)}: 175B parameters, few-shot learning
    \item Decoder-only architecture
    \item Pre-training objective: Next token prediction
    \item Emergence of in-context learning
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Other Notable Pre-trained Models}
\begin{itemize}
    \item \textbf{T5 (2019)}: Text-to-Text Transfer Transformer
    \item \textbf{RoBERTa (2019)}: Robustly Optimized BERT
    \item \textbf{ELECTRA (2020)}: More efficient pre-training
    \item \textbf{BART (2020)}: Denoising autoencoder
    \item Common trend: Scaling up model size and data
\end{itemize}
\end{frame}

% Section 4: Instruction Tuning
\section{Instruction Tuning and Alignment}
\begin{frame}
\frametitle{From Pre-training to Instruction Following}
\begin{itemize}
    \item Challenge: Pre-trained models don't naturally follow instructions
    \item \textbf{Instruction Tuning}: Fine-tuning on instruction-response pairs
    \item Key models:
    \begin{itemize}
        \item InstructGPT (2022)
        \item FLAN-T5 (2022)
        \item ChatGPT (2022)
    \end{itemize}
    \item Improved user interaction and task generalization
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{RLHF: Reinforcement Learning from Human Feedback}
\begin{itemize}
    \item Three-step process:
    \begin{enumerate}
        \item Supervised fine-tuning (SFT)
        \item Reward model training
        \item PPO optimization
    \end{enumerate}
    \item Aligning models with human preferences
    \item Key for safety and helpfulness
    \item Used in ChatGPT, Claude, and others
\end{itemize}
\end{frame}

% Section 5: Modern LLMs
\section{Modern Era: Large-Scale LLMs}
\begin{frame}
\frametitle{The Current Landscape (2023-2024)}
\begin{itemize}
    \item \textbf{GPT-4}: Multimodal capabilities
    \item \textbf{Claude 3}: Long context, strong reasoning
    \item \textbf{Gemini}: Google's multimodal model
    \item \textbf{LLaMA series}: Open-source foundation models
    \item Key trends:
    \begin{itemize}
        \item Multimodality (text, image, audio)
        \item Extended context windows
        \item Improved reasoning capabilities
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Open Source Revolution}
\begin{itemize}
    \item \textbf{LLaMA (2023)}: Meta's open foundation model
    \item \textbf{Mistral}: Efficient open models
    \item \textbf{Qwen}: Alibaba's multilingual models
    \item Community contributions:
    \begin{itemize}
        \item Alpaca, Vicuna (instruction-tuned variants)
        \item LoRA and efficient fine-tuning methods
        \item Quantization techniques (GGUF, AWQ)
    \end{itemize}
\end{itemize}
\end{frame}

% Section 6: Agents
\section{LLM Agents: The Frontier}
\begin{frame}
\frametitle{What are LLM Agents?}
\begin{itemize}
    \item \textbf{Definition}: LLMs augmented with:
    \begin{itemize}
        \item Tool/API usage capabilities
        \item Memory systems
        \item Planning and reasoning
        \item Action execution
    \end{itemize}
    \item Moving from passive responders to active problem solvers
    \item Key components: Perception, Reasoning, Action
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Agent Frameworks and Approaches}
\begin{itemize}
    \item \textbf{ReAct}: Reasoning + Acting paradigm
    \item \textbf{AutoGPT}: Autonomous task completion
    \item \textbf{LangChain/LangGraph}: Agent orchestration frameworks
    \item \textbf{Function Calling}: Direct API integration
    \item \textbf{Multi-agent systems}: Collaborative agents
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Agent Capabilities}
\begin{itemize}
    \item \textbf{Tool Use}:
    \begin{itemize}
        \item Web search, code execution
        \item Database queries, API calls
    \end{itemize}
    \item \textbf{Planning}:
    \begin{itemize}
        \item Task decomposition
        \item Multi-step reasoning
    \end{itemize}
    \item \textbf{Memory}:
    \begin{itemize}
        \item Short-term (context window)
        \item Long-term (vector databases)
    \end{itemize}
    \item \textbf{Self-reflection and error correction}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Real-World Agent Applications}
\begin{itemize}
    \item \textbf{Code Assistants}: GitHub Copilot, Cursor
    \item \textbf{Research Assistants}: Literature review, data analysis
    \item \textbf{Task Automation}: Workflow orchestration
    \item \textbf{Customer Service}: Intelligent chatbots
    \item \textbf{Scientific Discovery}: Drug discovery, theorem proving
\end{itemize}
\end{frame}

% Section 7: Challenges and Future
\section{Challenges and Future Directions}
\begin{frame}
\frametitle{Current Challenges}
\begin{itemize}
    \item \textbf{Technical Challenges}:
    \begin{itemize}
        \item Hallucinations and factual accuracy
        \item Context length limitations
        \item Computational costs
        \item Reliability and consistency
    \end{itemize}
    \item \textbf{Ethical Challenges}:
    \begin{itemize}
        \item Bias and fairness
        \item Privacy concerns
        \item Misuse and safety
        \item Environmental impact
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Future Directions}
\begin{itemize}
    \item \textbf{Efficiency}: Smaller, more capable models
    \item \textbf{Multimodality}: True unified understanding
    \item \textbf{Reasoning}: Enhanced logical and mathematical capabilities
    \item \textbf{Personalization}: Adaptive to individual users
    \item \textbf{Embodied AI}: Integration with robotics
    \item \textbf{Collaborative agents}: Multi-agent ecosystems
\end{itemize}
\end{frame}

% Conclusion
\section{Conclusion}
\begin{frame}
\frametitle{Summary: The Journey}
\begin{itemize}
    \item \textbf{2013-2017}: Word embeddings to Transformers
    \item \textbf{2018-2020}: Pre-training era (BERT, GPT)
    \item \textbf{2021-2022}: Scaling and instruction tuning
    \item \textbf{2023-2024}: Multimodal models and open source
    \item \textbf{2024-2026}: LLM Agents and autonomous systems
    \item \textbf{Future}: More capable, efficient, and aligned AI
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Key Takeaways}
\begin{itemize}
    \item LLMs evolved from simple word embeddings to sophisticated agents
    \item Pre-training and scaling were crucial breakthroughs
    \item Alignment with human intent is critical
    \item Agents represent the next frontier
    \item Both opportunities and challenges ahead
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Thank You!}
\begin{center}
\Large Questions?
\end{center}
\end{frame}

\end{document}
