% Part 2: Training and Inference
\section{Training and Inference}

\begin{frame}
\frametitle{Pre-training: Foundation Building}
\begin{itemize}
    \item \textbf{Objective}: Learn general language representations
    \item Large-scale unsupervised learning on web-scale corpora
    \item Common pre-training strategies:
    \begin{itemize}
        \item Causal language modeling (GPT)
        \item Masked language modeling (BERT)
        \item Span corruption (T5)
        \item Prefix language modeling (PaLM)
    \end{itemize}
    \item Training infrastructure:
    \begin{itemize}
        \item Distributed training across thousands of GPUs/TPUs
        \item Mixed precision and gradient accumulation
        \item Efficient optimizers (Adam, AdamW)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Post-training: Alignment and Specialization}
\begin{itemize}
    \item \textbf{Supervised Fine-tuning (SFT)}:
    \begin{itemize}
        \item High-quality instruction-response pairs
        \item Task-specific adaptation
        \item Improved user interaction
    \end{itemize}
    \item \textbf{Reinforcement Learning from Human Feedback (RLHF)}:
    \begin{itemize}
        \item Reward model training from human preferences
        \item PPO/DPO optimization
        \item Alignment with human values
    \end{itemize}
    \item \textbf{Other techniques}:
    \begin{itemize}
        \item Constitutional AI (Anthropic)
        \item RLAIF (AI feedback)
        \item Instruction tuning (FLAN, InstructGPT)
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multimodal Alignment}
\begin{itemize}
    \item Extending LLMs beyond text to vision, audio, video
    \item \textbf{Vision-Language Models}:
    \begin{itemize}
        \item CLIP: Contrastive learning for image-text alignment
        \item Flamingo, BLIP: Visual question answering
        \item GPT-4V, Gemini: Native multimodal understanding
    \end{itemize}
    \item \textbf{Alignment strategies}:
    \begin{itemize}
        \item Joint pre-training on multimodal data
        \item Cross-modal attention mechanisms
        \item Adapter modules for modality fusion
    \end{itemize}
    \item \textbf{Applications}: Image captioning, visual reasoning, video understanding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Inference Acceleration}
\begin{itemize}
    \item \textbf{Model compression}:
    \begin{itemize}
        \item Quantization (8-bit, 4-bit, GGUF, AWQ)
        \item Pruning and sparsity
        \item Knowledge distillation (smaller student models)
    \end{itemize}
    \item \textbf{Efficient architectures}:
    \begin{itemize}
        \item Mixture-of-Experts (MoE): Sparse activation
        \item FlashAttention: Memory-efficient attention
        \item Grouped-Query Attention (GQA)
    \end{itemize}
    \item \textbf{Serving optimizations}:
    \begin{itemize}
        \item KV-cache for autoregressive generation
        \item Batching and continuous batching
        \item Speculative decoding
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Reinforcement Learning in LLMs}
\begin{itemize}
    \item \textbf{RLHF pipeline}:
    \begin{enumerate}
        \item Collect human preference data
        \item Train reward model to predict preferences
        \item Optimize policy with RL (PPO, DPO)
    \end{enumerate}
    \item \textbf{Direct Preference Optimization (DPO)}:
    \begin{itemize}
        \item Simpler alternative to PPO
        \item Directly optimizes on preference data
        \item No separate reward model needed
    \end{itemize}
    \item \textbf{Advanced RL techniques}:
    \begin{itemize}
        \item Process-based reward modeling
        \item Outcome-based and trajectory-based rewards
        \item Multi-objective RL for competing goals
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficient Fine-tuning Methods}
\begin{itemize}
    \item \textbf{Parameter-Efficient Fine-Tuning (PEFT)}:
    \begin{itemize}
        \item LoRA (Low-Rank Adaptation): Add trainable low-rank matrices
        \item QLoRA: Quantized LoRA for memory efficiency
        \item Prefix tuning: Learn soft prompts
        \item Adapter layers: Small bottleneck modules
    \end{itemize}
    \item \textbf{Advantages}:
    \begin{itemize}
        \item Reduce memory and compute requirements
        \item Enable fine-tuning on consumer hardware
        \item Modular task-specific adaptations
    \end{itemize}
    \item Popular for domain adaptation and personalization
\end{itemize}
\end{frame}
