% Part 3: Agent Evolution
\section{Agent Evolution}

\begin{frame}
\frametitle{From Models to Agents}
\begin{itemize}
    \item \textbf{Traditional LLMs}: Passive text generators
    \item \textbf{LLM Agents}: Active problem solvers with:
    \begin{itemize}
        \item Perception (understanding environment)
        \item Planning (decomposing tasks)
        \item Action (executing operations)
        \item Memory (maintaining state)
    \end{itemize}
    \item Evolution: Prompting $\rightarrow$ RAG $\rightarrow$ Web Search $\rightarrow$ Tools \& MCP
    \item Goal: Autonomous task completion
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Stage 1: Model Prompting}
\begin{itemize}
    \item \textbf{Zero-shot prompting}: Direct task description
    \item \textbf{Few-shot prompting}: Examples in context
    \item \textbf{Chain-of-Thought (CoT)}:
    \begin{itemize}
        \item "Let's think step by step"
        \item Intermediate reasoning steps
        \item Improved complex problem solving
    \end{itemize}
    \item \textbf{Advanced prompting}:
    \begin{itemize}
        \item Self-consistency: Multiple reasoning paths
        \item Tree-of-Thoughts: Exploring multiple branches
        \item ReAct: Reasoning + Acting interleaved
    \end{itemize}
    \item Limitations: No external knowledge, no persistent memory
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Stage 2: RAG (Retrieval-Augmented Generation)}
\begin{itemize}
    \item \textbf{Concept}: Augment LLMs with external knowledge retrieval
    \item \textbf{Components}:
    \begin{itemize}
        \item Vector databases (Pinecone, Weaviate, Chroma)
        \item Embedding models for semantic search
        \item Retrieval mechanism for relevant context
    \end{itemize}
    \item \textbf{Workflow}:
    \begin{enumerate}
        \item User query $\rightarrow$ Embed query
        \item Retrieve top-k relevant documents
        \item Augment prompt with retrieved context
        \item Generate response with LLM
    \end{enumerate}
    \item \textbf{Benefits}: Access to dynamic knowledge, overcoming context limits
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Stage 3: Web Search Integration}
\begin{itemize}
    \item \textbf{Motivation}: LLMs trained on static data become outdated
    \item \textbf{Search-augmented LLMs}:
    \begin{itemize}
        \item Bing Chat, Perplexity AI
        \item Real-time information retrieval
        \item Citation and source tracking
    \end{itemize}
    \item \textbf{Workflow}:
    \begin{enumerate}
        \item Generate search query from user question
        \item Retrieve relevant web results
        \item Synthesize answer from search results
        \item Provide citations
    \end{enumerate}
    \item Challenges: Information quality, relevance filtering
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Stage 4: Tools and MCP}
\begin{itemize}
    \item \textbf{Tool-augmented LLMs}: Models that can use external tools
    \item \textbf{Function calling}:
    \begin{itemize}
        \item Define tools with schemas
        \item Model decides when and how to call
        \item Execute function and return results
    \end{itemize}
    \item \textbf{Model Context Protocol (MCP)}:
    \begin{itemize}
        \item Standard for connecting LLMs to data sources
        \item Unified interface for tools and integrations
        \item Extensible architecture
    \end{itemize}
    \item \textbf{Common tools}:
    \begin{itemize}
        \item Calculator, code interpreter
        \item Database queries, API calls
        \item Browser automation, file operations
    \end{itemize}
\end{itemize}
\end{frame}
